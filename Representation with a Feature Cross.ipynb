{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMD3oG7G9O/O4Se+rij/rED"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RViXcLtzsSI-"},"outputs":[],"source":["#@title Load the imports\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","from matplotlib import pyplot as plt\n","\n","# The following lines adjust the granularity of reporting.\n","pd.options.display.max_rows = 10\n","pd.options.display.float_format = \"{:.1f}\".format\n","\n","tf.keras.backend.set_floatx('float32')\n","\n","print(\"Imported the modules.\")"]},{"cell_type":"code","source":["# Load the dataset\n","train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n","test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")\n","\n","# Scale the labels\n","scale_factor = 1000.0\n","# Scale the training set's label.\n","train_df[\"median_house_value\"] /= scale_factor\n","\n","# Scale the test set's label\n","test_df[\"median_house_value\"] /= scale_factor\n","\n","# Shuffle the examples\n","train_df = train_df.reindex(np.random.permutation(train_df.index))"],"metadata":{"id":"WyAgpscWs_f-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Keras Input tensors of float values.\n","inputs = {\n","    'latitude':\n","        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n","                              name='latitude'),\n","    'longitude':\n","        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n","                              name='longitude')\n","}"],"metadata":{"id":"rhnyqszdtCQS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Define functions to create and train a model, and a plotting function\n","def create_model(my_inputs, my_outputs, my_learning_rate):\n","\n","  model = tf.keras.Model(inputs=my_inputs, outputs=my_outputs)\n","\n","  # Construct the layers into a model that TensorFlow can execute.\n","  model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(\n","      learning_rate=my_learning_rate),\n","      loss=\"mean_squared_error\",\n","      metrics=[tf.keras.metrics.RootMeanSquaredError()])\n","\n","  return model\n","\n","\n","def train_model(model, dataset, epochs, batch_size, label_name):\n","  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n","\n","  features = {name:np.array(value) for name, value in dataset.items()}\n","  label = np.array(features.pop(label_name))\n","  history = model.fit(x=features, y=label, batch_size=batch_size,\n","                      epochs=epochs, shuffle=True)\n","\n","  # The list of epochs is stored separately from the rest of history.\n","  epochs = history.epoch\n","\n","  # Isolate the mean absolute error for each epoch.\n","  hist = pd.DataFrame(history.history)\n","  rmse = hist[\"root_mean_squared_error\"]\n","\n","  return epochs, rmse\n","\n","\n","def plot_the_loss_curve(epochs, rmse):\n","  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n","\n","  plt.figure()\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Root Mean Squared Error\")\n","\n","  plt.plot(epochs, rmse, label=\"Loss\")\n","  plt.legend()\n","  plt.ylim([rmse.min()*0.94, rmse.max()* 1.05])\n","  plt.show()\n","\n","print(\"Defined the create_model, train_model, and plot_the_loss_curve functions.\")"],"metadata":{"id":"qCB_2FOOtFYC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The following variables are the hyperparameters.\n","learning_rate = 0.05\n","epochs = 30\n","batch_size = 100\n","label_name = 'median_house_value'\n","\n","# The two Input layers are concatenated so they can be passed as a single\n","# tensor to a Dense layer.\n","preprocessing_layer = tf.keras.layers.Concatenate()(inputs.values())\n","\n","dense_output = layers.Dense(\n","    units=1,\n","    input_shape=(1,),\n","    name='dense_layer')(preprocessing_layer)\n","\n","outputs = {\n","  'dense_output': dense_output\n","}\n","\n","# Create and compile the model's topography.\n","my_model = create_model(inputs, outputs, learning_rate)\n","\n","# To view a PNG of this model's layers, uncomment the call to\n","# `tf.keras.utils.plot_model` below. After running this code cell, click\n","# the file folder on the left, then the `my_model.png` file.\n","# tf.keras.utils.plot_model(my_model, \"my_model.png\", show_shapes=True)\n","\n","# Train the model on the training set.\n","epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n","\n","# Print out the model summary.\n","my_model.summary(expand_nested=True)\n","\n","plot_the_loss_curve(epochs, rmse)\n","\n","print(\"\\n: Evaluate the new model against the test set:\")\n","test_features = {name:np.array(value) for name, value in test_df.items()}\n","test_label = np.array(test_features.pop(label_name))\n","my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)"],"metadata":{"id":"n3UuMPQWtIdL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Representing latitude and longitude as\n","# floating-point values does not have much\n","# predictive power. For example, neighborhoods at\n","# latitude 35 are not 36/35 more valuable\n","# (or 35/36 less valuable) than houses at\n","# latitude 36.\n","\n","# Representing `latitude` and `longitude` as\n","# floating-point values provides almost no\n","# predictive power. We're only using the raw values\n","# to establish a baseline for future experiments\n","# with better representations."],"metadata":{"id":"N6c4nKkktLCh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["resolution_in_degrees = 1.0\n","\n","# Create a list of numbers representing the bucket boundaries for latitude.\n","latitude_boundaries = list(np.arange(int(min(train_df['latitude'])),\n","                                     int(max(train_df['latitude'])),\n","                                     resolution_in_degrees))\n","print(\"latitude boundaries: \" + str(latitude_boundaries))\n","\n","# Create a Discretization layer to separate the latitude data into buckets.\n","latitude = tf.keras.layers.Discretization(\n","    bin_boundaries=latitude_boundaries,\n","    name='discretization_latitude')(inputs.get('latitude'))\n","\n","# Number of categories is the length of latitude_boundaries plus one.\n","latitude = tf.keras.layers.CategoryEncoding(\n","    num_tokens=len(latitude_boundaries) + 1,\n","    output_mode='one_hot',\n","    name='category_encoding_latitude')(latitude)\n","\n","# Create a list of numbers representing the bucket boundaries for longitude.\n","longitude_boundaries = list(np.arange(int(min(train_df['longitude'])),\n","                                      int(max(train_df['longitude'])),\n","                                      resolution_in_degrees))\n","\n","print(\"longitude boundaries: \" + str(longitude_boundaries))\n","\n","# Create a Discretization layer to separate the longitude data into buckets.\n","longitude = tf.keras.layers.Discretization(\n","    bin_boundaries=longitude_boundaries,\n","    name='discretization_longitude')(inputs.get('longitude'))\n","\n","# Number of categories is the length of longitude_boundaries plus one.\n","longitude = tf.keras.layers.CategoryEncoding(\n","    num_tokens=len(longitude_boundaries) + 1,\n","    output_mode='one_hot',\n","    name='category_encoding_longitude')(longitude)\n","\n","# Concatenate latitude and longitude into a single tensor as input for the Dense layer.\n","concatenate_layer = tf.keras.layers.Concatenate()([latitude, longitude])\n","\n","dense_output = layers.Dense(\n","    units=1, input_shape=(2,), name='dense_layer')(concatenate_layer)\n","\n","# Define an output dictionary we'll send to the model constructor.\n","outputs = {\n","  'dense_output': dense_output\n","}"],"metadata":{"id":"OcIhaArptP6f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The following variables are the hyperparameters.\n","learning_rate = 0.04\n","epochs = 35\n","\n","# Build the model.\n","my_model = create_model(inputs, outputs, learning_rate)\n","\n","# Train the model on the training set.\n","epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n","\n","# Print out the model summary.\n","my_model.summary(expand_nested=True)\n","\n","plot_the_loss_curve(epochs, rmse)\n","\n","print(\"\\n: Evaluate the new model against the test set:\")\n","my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)"],"metadata":{"id":"Xs1qY-7MtXwC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Bucket representation outperformed\n","# floating-point representations.\n","# However, you can still do far better."],"metadata":{"id":"ndCpGJ8CtbWg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Representing location as a feature cross should\n","# produce better results.\n","\n","# Real-world locations, however, exist in\n","# two dimensions. Therefore, you should\n","# represent location as a two-dimensional feature\n","# cross. That is, you'll cross the 10 or so latitude\n","# buckets with the 10 or so longitude buckets to\n","# create a grid of 100 cells.\n","\n","# The model will learn separate weights for each\n","# of the cells."],"metadata":{"id":"nMMRIsEXtfcN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["resolution_in_degrees = 1.0\n","\n","# Create a list of numbers representing the bucket boundaries for latitude.\n","latitude_boundaries = list(np.arange(int(min(train_df['latitude'])),\n","                                     int(max(train_df['latitude'])),\n","                                     resolution_in_degrees))\n","\n","# Create a Discretization layer to separate the latitude data into buckets.\n","latitude = tf.keras.layers.Discretization(\n","    bin_boundaries=latitude_boundaries,\n","    name='discretization_latitude')(inputs.get('latitude'))\n","\n","# Create a list of numbers representing the bucket boundaries for longitude.\n","longitude_boundaries = list(np.arange(int(min(train_df['longitude'])),\n","                                      int(max(train_df['longitude'])),\n","                                      resolution_in_degrees))\n","\n","# Create a Discretization layer to separate the longitude data into buckets.\n","longitude = tf.keras.layers.Discretization(\n","    bin_boundaries=longitude_boundaries,\n","    name='discretization_longitude')(inputs.get('longitude'))\n","\n","# Cross the latitude and longitude features into a single one-hot vector.\n","feature_cross = tf.keras.layers.HashedCrossing(\n","    num_bins=len(latitude_boundaries) * len(longitude_boundaries),\n","    output_mode='one_hot',\n","    name='cross_latitude_longitude')([latitude, longitude])\n","\n","dense_output = layers.Dense(units=1, input_shape=(2,),\n","                            name='dense_layer')(feature_cross)\n","\n","# Define an output dictionary we'll send to the model constructor.\n","outputs = {\n","  'dense_output': dense_output\n","}"],"metadata":{"id":"mYW0yvsgtpth"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The following variables are the hyperparameters.\n","learning_rate = 0.04\n","epochs = 35\n","\n","# Build the model, this time passing in the feature_cross_feature_layer:\n","my_model = create_model(inputs, outputs, learning_rate)\n","\n","# Train the model on the training set.\n","epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n","\n","# Print out the model summary.\n","my_model.summary(expand_nested=True)\n","\n","plot_the_loss_curve(epochs, rmse)\n","\n","print(\"\\n: Evaluate the new model against the test set:\")\n","my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)"],"metadata":{"id":"BlKPHEOAtscv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Representing these features as a feature\n","# cross produced much lower loss values than\n","# representing these features as buckets"],"metadata":{"id":"AIJ1oZ0dtwBo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#   A resolution of ~0.4 degree provides the best\n","#   results.\n","\n","#   Below ~0.4 degree, loss increases because the\n","#   dataset does not contain enough examples in\n","#   each cell to accurately predict prices for\n","#   those cells.\n","\n","#   Postal code would be a far better feature\n","#   than latitude X longitude, assuming that\n","#   the dataset contained sufficient examples\n","#   in each postal code."],"metadata":{"id":"Fj-lmpNhtyXy"},"execution_count":null,"outputs":[]}]}